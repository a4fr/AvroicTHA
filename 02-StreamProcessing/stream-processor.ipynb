{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration and SparcSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, count, sum, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for Hadoop in Windows\n",
    "HADOOP_HOME = os.getenv(\"HADOOP_HOME\")\n",
    "if (not HADOOP_HOME) and (os.name == \"nt\"):\n",
    "    os.environ[\"HADOOP_HOME\"] = r\"C:\\winutils-master\\hadoop-3.3.6\"\n",
    "    HADOOP_HOME = os.getenv(\"HADOOP_HOME\")\n",
    "    os.environ[\"PATH\"] = fr\"{os.environ[\"PATH\"]};{HADOOP_HOME}\\bin\"\n",
    "    print(\"Hadoop Native Libraries configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for Kafka\n",
    "KAFKA_BROKER = os.getenv(\"KAFKA_BROKER\", \"localhost:9092, broker:29092\")\n",
    "KAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\", \"avroic\")\n",
    "KAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\", \"spark-aggregator\")\n",
    "DEPLOY_MODE = os.getenv(\"DEPLOY_MODE\", \"client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"InteractionsAggregator\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\") \\\n",
    "    .config(\"spark.submit.deployMode\", DEPLOY_MODE) \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"avroic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Kafka messages Schema\n",
    "schema = StructType([\n",
    "    StructField('user_id', StringType()),\n",
    "    StructField('item_id', StringType()),\n",
    "    StructField('interaction_type', StringType()),\n",
    "    StructField('timestamp', TimestampType())\n",
    "])\n",
    "\n",
    "# Parse Kafka messages\n",
    "interactions = df \\\n",
    "    .select(from_json(col('value').cast('string'), schema).alias('data')) \\\n",
    "    .select('data.*')\n",
    "\n",
    "interactions.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"./spark-table-checkpoint/interactions\") \\\n",
    "    .toTable(\"interactions\")\n",
    "\n",
    "print(\"Reading from Kafka...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from interactions order by timestamp desc limit 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg_interactions_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "    with agg_interactions_user_interaction_type as (\n",
    "        select user_id,\n",
    "               interaction_type,\n",
    "               count(*) as total_interactions,\n",
    "               max(timestamp) as last_interaction\n",
    "        from interactions\n",
    "        group by user_id, interaction_type\n",
    "        order by user_id, interaction_type\n",
    "    )\n",
    "    select user_id,\n",
    "           sum(case when interaction_type='click' then total_interactions else 0 end) as total_click,\n",
    "           sum(case when interaction_type='like' then total_interactions else 0 end) as total_like,\n",
    "           sum(case when interaction_type='view' then total_interactions else 0 end) as total_view,\n",
    "           sum(case when interaction_type='purchase' then total_interactions else 0 end) as total_purchase,\n",
    "           round(avg(total_interactions), 2) as avg_interactions,\n",
    "           sum(total_interactions) as total_interactions,\n",
    "           max(last_interaction) as last_interaction\n",
    "    from agg_interactions_user_interaction_type\n",
    "    group by user_id\n",
    "    order by total_interactions desc\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.createOrReplaceTempView(\"agg_interactions_user\")\n",
    "result.show()\n",
    "\n",
    "# result.write.parquet(\"output/agg_interactions_user.parquet\", mode=\"overwrite\")\n",
    "# print(\"Result saved as parquet!\")\n",
    "\n",
    "result.selectExpr(\"CONCAT('agg_interactions_user-', CAST(user_id AS STRING)) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"topic\", f\"{KAFKA_TOPIC}_aggregated\") \\\n",
    "    .save()\n",
    "print(\"Result sent to Kafka!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avg_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "    select round(avg(total_interactions), 2) as avg_interactions,\n",
    "           max(last_interaction) as last_interaction\n",
    "    from agg_interactions_user\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.show()\n",
    "\n",
    "result.selectExpr(\"'avg_interactions' AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"topic\", f\"{KAFKA_TOPIC}_aggregated\") \\\n",
    "    .save()\n",
    "print(\"Result sent to Kafka!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg_interactions_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "    select min(total_click) as min_click,\n",
    "           max(total_click) as max_click,\n",
    "           min(total_like) as min_like,\n",
    "           max(total_like) as max_like,\n",
    "           min(total_view) as min_view,\n",
    "           max(total_view) as max_view,\n",
    "           min(total_purchase) as min_purchase,\n",
    "           max(total_purchase) as max_purchase\n",
    "    from agg_interactions_user\n",
    "\"\"\"\n",
    "sql_query = \"\"\"\n",
    "    with agg_interactions_item_interaction_type as (\n",
    "        select item_id,\n",
    "            interaction_type,\n",
    "            count(*) as total_interactions\n",
    "        from interactions\n",
    "        group by item_id, interaction_type\n",
    "        order by item_id, interaction_type\n",
    "    )\n",
    "    select item_id,\n",
    "        max(total_interactions) as max_interactions,\n",
    "        min(total_interactions) as min_interactions\n",
    "    from agg_interactions_item_interaction_type\n",
    "    group by item_id\n",
    "    order by item_id\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.show()\n",
    "\n",
    "result.selectExpr(\"CONCAT('agg_interactions_item-', CAST(item_id AS STRING)) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"topic\", f\"{KAFKA_TOPIC}_aggregated\") \\\n",
    "    .save()\n",
    "print(\"Result sent to Kafka!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
