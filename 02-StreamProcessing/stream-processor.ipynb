{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration and SparcSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, count, sum, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop Native Libraries configured!\n"
     ]
    }
   ],
   "source": [
    "# Configs for Hadoop in Windows\n",
    "HADOOP_HOME = os.getenv(\"HADOOP_HOME\")\n",
    "if (not HADOOP_HOME) and (os.name == \"nt\"):\n",
    "    os.environ[\"HADOOP_HOME\"] = r\"C:\\winutils-master\\hadoop-3.3.6\"\n",
    "    HADOOP_HOME = os.getenv(\"HADOOP_HOME\")\n",
    "    os.environ[\"PATH\"] = fr\"{os.environ[\"PATH\"]};{HADOOP_HOME}\\bin\"\n",
    "    print(\"Hadoop Native Libraries configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for Kafka\n",
    "KAFKA_BROKER = os.getenv(\"KAFKA_BROKER\", \"localhost:9092, broker:29092\")\n",
    "KAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\", \"avroic\")\n",
    "KAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\", \"spark-aggregator\")\n",
    "DEPLOY_MODE = os.getenv(\"DEPLOY_MODE\", \"client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"InteractionsAggregator\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\") \\\n",
    "    .config(\"spark.submit.deployMode\", DEPLOY_MODE) \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from Kafka...\n"
     ]
    }
   ],
   "source": [
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", \"avroic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Kafka messages Schema\n",
    "schema = StructType([\n",
    "    StructField('user_id', StringType()),\n",
    "    StructField('item_id', StringType()),\n",
    "    StructField('interaction_type', StringType()),\n",
    "    StructField('timestamp', TimestampType())\n",
    "])\n",
    "\n",
    "# Parse Kafka messages\n",
    "interactions = df \\\n",
    "    .select(from_json(col('value').cast('string'), schema).alias('data')) \\\n",
    "    .select('data.*')\n",
    "\n",
    "interactions.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"./spark-table-checkpoint/interactions\") \\\n",
    "    .toTable(\"interactions\")\n",
    "\n",
    "interactions.createOrReplaceTempView(\"interactions_temp\")\n",
    "\n",
    "print(\"Reading from Kafka...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from interactions\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg_interactions_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+----------+--------------+----------------+------------------+-------------------+\n",
      "|user_id|total_click|total_like|total_view|total_purchase|avg_interactions|total_interactions|   last_interaction|\n",
      "+-------+-----------+----------+----------+--------------+----------------+------------------+-------------------+\n",
      "| user_9|          7|         3|         5|             2|            4.25|                17|2025-01-10 14:34:32|\n",
      "| user_8|          4|         6|         3|             3|             4.0|                16|2025-01-10 14:08:42|\n",
      "| user_3|          6|         3|         4|             2|            3.75|                15|2025-01-10 14:08:41|\n",
      "| user_7|          6|         1|         4|             2|            3.25|                13|2025-01-10 14:08:44|\n",
      "| user_5|          2|         3|         3|             2|             2.5|                10|2025-01-10 14:08:40|\n",
      "| user_0|          1|         5|         3|             1|             2.5|                10|2025-01-10 14:08:38|\n",
      "| user_6|          3|         1|         0|             6|            3.33|                10|2025-01-10 14:08:33|\n",
      "| user_1|          2|         0|         3|             4|             3.0|                 9|2025-01-10 14:08:21|\n",
      "| user_4|          1|         1|         2|             4|             2.0|                 8|2025-01-10 14:08:35|\n",
      "| user_2|          3|         1|         2|             2|             2.0|                 8|2025-01-10 14:08:46|\n",
      "+-------+-----------+----------+----------+--------------+----------------+------------------+-------------------+\n",
      "\n",
      "Result sent to Kafka!\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "    with agg_interactions_user_interaction_type as (\n",
    "        select user_id,\n",
    "               interaction_type,\n",
    "               count(*) as total_interactions,\n",
    "               max(timestamp) as last_interaction\n",
    "        from interactions\n",
    "        group by user_id, interaction_type\n",
    "        order by user_id, interaction_type\n",
    "    )\n",
    "    select user_id,\n",
    "           sum(case when interaction_type='click' then total_interactions else 0 end) as total_click,\n",
    "           sum(case when interaction_type='like' then total_interactions else 0 end) as total_like,\n",
    "           sum(case when interaction_type='view' then total_interactions else 0 end) as total_view,\n",
    "           sum(case when interaction_type='purchase' then total_interactions else 0 end) as total_purchase,\n",
    "           round(avg(total_interactions), 2) as avg_interactions,\n",
    "           sum(total_interactions) as total_interactions,\n",
    "           max(last_interaction) as last_interaction\n",
    "    from agg_interactions_user_interaction_type\n",
    "    group by user_id\n",
    "    order by total_interactions desc\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.createOrReplaceTempView(\"agg_interactions_user\")\n",
    "result.show()\n",
    "\n",
    "# result.write.parquet(\"output/agg_interactions_user.parquet\", mode=\"overwrite\")\n",
    "# print(\"Result saved as parquet!\")\n",
    "\n",
    "result.selectExpr(\"CONCAT('agg_interactions_user-', CAST(user_id AS STRING)) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"topic\", f\"{KAFKA_TOPIC}_aggregated\") \\\n",
    "    .save()\n",
    "print(\"Result sent to Kafka!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avg_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|avg_interactions|   last_interaction|\n",
      "+----------------+-------------------+\n",
      "|            11.6|2025-01-10 14:34:32|\n",
      "+----------------+-------------------+\n",
      "\n",
      "Result sent to Kafka!\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "    select round(avg(total_interactions), 2) as avg_interactions,\n",
    "           max(last_interaction) as last_interaction\n",
    "    from agg_interactions_user\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.show()\n",
    "\n",
    "result.selectExpr(\"'avg_interactions' AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"topic\", f\"{KAFKA_TOPIC}_aggregated\") \\\n",
    "    .save()\n",
    "print(\"Result sent to Kafka!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg_interactions_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------------+\n",
      "|item_id|max_interactions|min_interactions|\n",
      "+-------+----------------+----------------+\n",
      "| item_0|               4|               3|\n",
      "| item_1|               5|               2|\n",
      "| item_2|               7|               1|\n",
      "| item_3|               4|               1|\n",
      "| item_4|               2|               1|\n",
      "| item_5|               4|               1|\n",
      "| item_6|               7|               1|\n",
      "| item_7|               5|               1|\n",
      "| item_8|               4|               2|\n",
      "| item_9|               5|               2|\n",
      "+-------+----------------+----------------+\n",
      "\n",
      "Result sent to Kafka!\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "    select min(total_click) as min_click,\n",
    "           max(total_click) as max_click,\n",
    "           min(total_like) as min_like,\n",
    "           max(total_like) as max_like,\n",
    "           min(total_view) as min_view,\n",
    "           max(total_view) as max_view,\n",
    "           min(total_purchase) as min_purchase,\n",
    "           max(total_purchase) as max_purchase\n",
    "    from agg_interactions_user\n",
    "\"\"\"\n",
    "sql_query = \"\"\"\n",
    "    with agg_interactions_item_interaction_type as (\n",
    "        select item_id,\n",
    "            interaction_type,\n",
    "            count(*) as total_interactions\n",
    "        from interactions\n",
    "        group by item_id, interaction_type\n",
    "        order by item_id, interaction_type\n",
    "    )\n",
    "    select item_id,\n",
    "        max(total_interactions) as max_interactions,\n",
    "        min(total_interactions) as min_interactions\n",
    "    from agg_interactions_item_interaction_type\n",
    "    group by item_id\n",
    "    order by item_id\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.show()\n",
    "\n",
    "result.selectExpr(\"CONCAT('agg_interactions_item-', CAST(item_id AS STRING)) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"topic\", f\"{KAFKA_TOPIC}_aggregated\") \\\n",
    "    .save()\n",
    "print(\"Result sent to Kafka!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
